{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.dataScaling import data_scaler\n",
    "\n",
    "# %%\n",
    "# dataPath = 'src/H2DB.h5'\n",
    "dataPath = 'src/H2DB_L.h5'\n",
    "\n",
    "ddOrg = dd.read_hdf(dataPath, key='c')\n",
    "ddWdot = dd.read_hdf(dataPath, key='wdot')\n",
    "\n",
    "# %%\n",
    "input_features = [\n",
    "    'H', 'H2', 'O', 'O2', 'OH', 'H2O', 'N2', 'HO2', 'H2O2', 'Hs', 'Temp'\n",
    "]\n",
    "labels = input_features\n",
    "\n",
    "org = ddOrg.compute()\n",
    "wdot = ddWdot.compute()\n",
    "\n",
    "\n",
    "# %%\n",
    "def read_h5_data(input_features, labels):\n",
    "    in_scaler = data_scaler()\n",
    "    out_scaler = data_scaler()\n",
    "    input_df = org[input_features]\n",
    "    input_np = in_scaler.fit_transform(input_df[input_features].values, 'std2')\n",
    "\n",
    "    # label_df = ((new[labels]-old[labels]).div((org.dt+old.dt), axis=0))\n",
    "    label_df = wdot[labels]\n",
    "    label_np = out_scaler.fit_transform(label_df[labels].values, 'std2')\n",
    "\n",
    "    return input_np, label_np, in_scaler, out_scaler\n",
    "\n",
    "\n",
    "x_input, y_label, in_scaler, out_scaler = read_h5_data(\n",
    "    input_features=input_features, labels=labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_input,\n",
    "                                                    y_label,\n",
    "                                                    test_size=0.05)\n",
    "pickle.dump((labels, in_scaler, out_scaler), open('./data/tmp.pkl', 'wb'))\n",
    "\n",
    "#%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "TensorFlow version:  2.0.0-dev20190715\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not tf.test.is_gpu_available():\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up ANN\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2a (Dense)          (None, 192)          12480       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2a_0 (Dense)        (None, 192)          12480       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2a_1 (Dense)        (None, 192)          12480       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2a_2 (Dense)        (None, 192)          12480       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2a_3 (Dense)        (None, 192)          12480       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 192)          0           res1a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 192)          0           res1a_branch2a_0[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 192)          0           res1a_branch2a_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 192)          0           res1a_branch2a_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 192)          0           res1a_branch2a_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 960)          0           activation[0][0]                 \n",
      "                                                                 activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2b (Dense)          (None, 64)           61504       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64)           0           res1a_branch2b[0][0]             \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "res1b_branch2a (Dense)          (None, 192)          12480       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res1b_branch2a_0 (Dense)        (None, 192)          12480       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res1b_branch2a_1 (Dense)        (None, 192)          12480       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res1b_branch2a_2 (Dense)        (None, 192)          12480       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res1b_branch2a_3 (Dense)        (None, 192)          12480       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 192)          0           res1b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 192)          0           res1b_branch2a_0[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 192)          0           res1b_branch2a_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 192)          0           res1b_branch2a_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 192)          0           res1b_branch2a_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 960)          0           activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_9[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res1b_branch2b (Dense)          (None, 64)           61504       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64)           0           res1b_branch2b[0][0]             \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64)           0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          6500        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dense)                (None, 11)           1111        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 256,187\n",
      "Trainable params: 256,187\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training\n",
      "mkdir: cannot create directory ‘./tmp’: File exists\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 16:13:21.780362 140016731551552 deprecation.py:323] From /home/edison/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:460: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16136997 samples, validate on 1793000 samples\n",
      "Epoch 1/6\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00000, saving model to ./tmp/n64_b5_fcTrue.weights.best.cntk.hdf5\n",
      "16136997/16136997 - 41s - loss: 9.8225e-05 - accuracy: 0.9935 - val_loss: 3.2176e-06 - val_accuracy: 0.9965\n",
      "Epoch 2/6\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00000\n",
      "16136997/16136997 - 40s - loss: 1.0229e-05 - accuracy: 0.9963 - val_loss: 1.1411e-04 - val_accuracy: 0.9956\n",
      "Epoch 3/6\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "16136997/16136997 - 40s - loss: 2.1324e-05 - accuracy: 0.9957 - val_loss: 1.7143e-05 - val_accuracy: 0.9949\n",
      "Epoch 4/6\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "16136997/16136997 - 39s - loss: 3.0291e-05 - accuracy: 0.9954 - val_loss: 5.1116e-05 - val_accuracy: 0.9942\n",
      "Epoch 5/6\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "16136997/16136997 - 40s - loss: 2.0602e-05 - accuracy: 0.9953 - val_loss: 1.1229e-04 - val_accuracy: 0.9937\n",
      "Epoch 6/6\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "16136997/16136997 - 40s - loss: 3.3743e-05 - accuracy: 0.9950 - val_loss: 1.2217e-05 - val_accuracy: 0.9957\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from src.dataScaling import data_scaler\n",
    "from src.res_block import res_block\n",
    "from src.utils import SGDRScheduler\n",
    "\n",
    "\n",
    "# %%\n",
    "print('set up ANN')\n",
    "\n",
    "n_neuron = 100\n",
    "scale = 3\n",
    "branches = 3\n",
    "fc=True\n",
    "\n",
    "for n_neuron in [64]:\n",
    "    for branches in [5]:\n",
    "        for fc in [True]:\n",
    "            m_name='n{}_b{}_fc{}'.format(n_neuron,branches,fc)\n",
    "            dim_input = x_train.shape[1]\n",
    "            dim_label = y_train.shape[1]\n",
    "\n",
    "            batch_norm = False\n",
    "\n",
    "            # strategy = tensorflow.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "            # print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "            # with strategy.scope():\n",
    "            inputs = Input(shape=(dim_input,), name='input_1')\n",
    "            x = Dense(n_neuron, activation='relu')(inputs)\n",
    "\n",
    "            # less then 2 res_block, there will be variance\n",
    "            x = res_block(x, scale, n_neuron, stage=1, block='a',\n",
    "                        bn=batch_norm, branches=branches)\n",
    "            x = res_block(x, scale, n_neuron, stage=1, block='b',\n",
    "                        bn=batch_norm, branches=branches)\n",
    "            # x = res_block(x, scale, n_neuron, stage=1, block='c', bn=batch_norm,branches=branches)\n",
    "\n",
    "            if fc ==True:\n",
    "                x = Dense(100, activation='relu')(x)\n",
    "            # x = Dropout(0.1)(x)\n",
    "            predictions = Dense(dim_label, activation='linear', name='output_1')(x)\n",
    "\n",
    "            model =  Model(inputs=inputs, outputs=predictions)\n",
    "            model.summary()\n",
    "\n",
    "            loss_type = 'mse'\n",
    "            model.compile(loss=loss_type, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            # %%\n",
    "            print('Training')\n",
    "            batch_size = 1024*8*8\n",
    "            epochs = 400\n",
    "            vsplit = 0.1\n",
    "            # model.compile(loss=loss_type, optimizer='adam', metrics=[coeff_r2])\n",
    "\n",
    "            # checkpoint (save the best model based validate loss)\n",
    "            !mkdir ./tmp\n",
    "            filepath = \"./tmp/{}.weights.best.cntk.hdf5\".format(m_name)\n",
    "\n",
    "            checkpoint = ModelCheckpoint(filepath,\n",
    "                                        monitor='val_loss',\n",
    "                                        verbose=1,\n",
    "                                        save_best_only=True,\n",
    "                                        mode='min',\n",
    "                                        save_freq='epoch')\n",
    "                                        # period = 10)\n",
    "\n",
    "            epoch_size = x_train.shape[0]\n",
    "            ep_size = 0\n",
    "            base = 2\n",
    "            clc = 2\n",
    "            for i in range(2):\n",
    "                ep_size += base*clc**(i)\n",
    "            print(ep_size)\n",
    "            epochs, c_len = ep_size, base\n",
    "            schedule = SGDRScheduler(min_lr=1e-6, max_lr=1e-4,\n",
    "                                    steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                    cycle_length=c_len, lr_decay=0.8, mult_factor=2)\n",
    "\n",
    "            callbacks_list1 = [checkpoint, tensorflow.keras.callbacks.TensorBoard('./tb/{}'.format(m_name),histogram_freq=1, profile_batch = 3)]\n",
    "\n",
    "            model.load_weights(filepath)\n",
    "\n",
    "            # fit the model\n",
    "            history = model.fit(\n",
    "                x_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=vsplit,\n",
    "                verbose=2,\n",
    "                callbacks=callbacks_list1,\n",
    "                shuffle=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
